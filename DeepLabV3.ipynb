{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pPFSQkgGSe7S"
      },
      "source": [
        "## Multiclass semantic segmentation using DeepLabV3+ on XView2\n",
        "\n",
        "**Author:** [Kayla Aky√ºz](https://github.com/kaylaa0)<br>\n",
        "**Date created:** 2023/06/14<br>\n",
        "**Last modified:** 2023/06/17<br>\n",
        "**Description:** Implement DeepLabV3+ architecture for Multi-class Semantic Segmentation on XView2 dataset with test and evaluation methods. The notebook is derived from [Soumik Rakshit's](http://github.com/soumik12345) [Notebook](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/deeplabv3_plus.ipynb) and [Keras Article](https://keras.io/examples/vision/deeplabv3_plus/)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BMC4PLipSe7Z"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Semantic segmentation, with the goal to assign semantic labels to every pixel in an image,\n",
        "is an essential computer vision task. In this example, we implement\n",
        "the **DeepLabV3+** model for multi-class semantic segmentation, a fully-convolutional\n",
        "architecture that performs well on semantic segmentation benchmarks.\n",
        "\n",
        "### References:\n",
        "\n",
        "- [Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](https://arxiv.org/pdf/1802.02611.pdf)\n",
        "- [Rethinking Atrous Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587)\n",
        "- [DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs](https://arxiv.org/abs/1606.00915)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dxedulEhSe7b"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "We will use the [xView2 xBD dataset](https://xview2.org/download-links)\n",
        "for training our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnTGQ0SDSe7c"
      },
      "outputs": [],
      "source": [
        "#@title Import necessary libraries. { display-mode: \"form\" }\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import shutil\n",
        "import json\n",
        "from PIL import Image, ImageDraw\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import random\n",
        "from random import randrange\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.callbacks import CSVLogger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJhw_bxnVxi3"
      },
      "outputs": [],
      "source": [
        "#@title Define data directory. { display-mode: \"form\" }\n",
        "colab_drive = \"./\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8hYjjJaUDhc"
      },
      "outputs": [],
      "source": [
        "#@title Select to use only CPU, avoids OOM but slower. { display-mode: \"form\" }\n",
        "CPU_ONLY = False #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "if CPU_ONLY:\n",
        "    cpus = tf.config.experimental.list_physical_devices('CPU')\n",
        "    tf.config.set_visible_devices([], 'GPU')  # hide the GPU\n",
        "    tf.config.set_visible_devices(cpus[0], 'CPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iYHP_U4IgGqD"
      },
      "outputs": [],
      "source": [
        "#@title Mount `Google Drive`. { display-mode: \"form\" }\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "colab_drive = \"/content/gdrive/My Drive/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyvCqV7HgLca"
      },
      "outputs": [],
      "source": [
        "#@title Insert the download link from `XView2.org` to save a temp directory. { display-mode: \"form\" }\n",
        "link = 'Insert download link' #@param {type:\"string\"}\n",
        "!wget {link} -P \"/content/Xview2/root\" -O \"train_images_labels_targets.tar.gz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rd01gcRhs0T"
      },
      "outputs": [],
      "source": [
        "#@title Unzip the contents. { display-mode: \"form\" }\n",
        "!tar -xzvf \"/content/Xview2/root/train_images_labels_targets.tar.gz\" -C \"/content/train/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_NlDQ5jpCsX"
      },
      "outputs": [],
      "source": [
        "#@title Process train images and save to `Google Drive`. { display-mode: \"form\" }\n",
        "convertTif = False #@param {type:\"boolean\"}\n",
        "input_dir = \"/content/train/train/images/\"\n",
        "output_dir = \"Xview2/imagesdeeplab/\" #@param {type:\"string\"}\n",
        "output_dir = colab_drive+output_dir\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def tiff_to_png(input_path, output_dir):\n",
        "    # Read the TIFF image\n",
        "    tiff_image = imageio.imread(input_path)\n",
        "\n",
        "    # Convert to PNG format\n",
        "    tiff_image = tiff_image.astype(np.uint8)\n",
        "    # Create the output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Get the filename without extension\n",
        "    filename = os.path.splitext(os.path.basename(input_path))[0]\n",
        "\n",
        "    # Save the PNG image to the output directory\n",
        "    output_path = os.path.join(output_dir, f\"{filename}.png\")\n",
        "    imageio.imwrite(output_path, tiff_image)\n",
        "\n",
        "# Example usage\n",
        "if convertTif:\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.endswith('.tif'):\n",
        "            tiff_to_png(os.path.join(input_dir, filename), output_dir)\n",
        "else:\n",
        "    shutil.copy(input_dir, colab_drive+\"Xview2/images/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_M0Z6w2pVnL"
      },
      "outputs": [],
      "source": [
        "#@title Process train labels and save to `Google Drive`. { display-mode: \"form\" }\n",
        "# Set input and output directories\n",
        "input_dir = '/content/train/train/labels/'\n",
        "output_dir = \"Xview2/labelsdeeplab\"  #@param {type:\"string\"}\n",
        "output_dir = colab_drive+output_dir\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Define color mappings for feature type and subtype\n",
        "\n",
        "def get_class_number(feature_type, subtype=None):\n",
        "    # Define the mapping of feature types and subtypes to class numbers\n",
        "    class_mapping = {\n",
        "        'building': 1,\n",
        "    }\n",
        "\n",
        "    # Define the mapping of subtypes to class numbers (if applicable)\n",
        "    subtype_mapping = {\n",
        "        'no-damage': 2,\n",
        "        'minor-damage': 3,\n",
        "        'destroyed': 4,\n",
        "        'major-damage': 5,\n",
        "        'un-classified': 6\n",
        "    }\n",
        "\n",
        "    # Check if the feature has a subtype\n",
        "    if subtype is not None:\n",
        "        # Return the class number based on feature type and subtype\n",
        "        return subtype_mapping.get(subtype, 0)  # Assign a default value of 0 if subtype is not found\n",
        "\n",
        "    # Return the class number based on feature type\n",
        "    return class_mapping.get(feature_type, 0)  # Assign a default value of 0 if feature type is not found\n",
        "\n",
        "# Process each JSON label file in the input directory\n",
        "for filename in os.listdir(input_dir):\n",
        "    if filename.endswith('.json'):\n",
        "        # Load JSON data from file\n",
        "        with open(os.path.join(input_dir, filename), 'r') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        # Set image size\n",
        "        image_width = data['metadata']['width']\n",
        "        image_height = data['metadata']['height']\n",
        "\n",
        "        # Create a new image with white background\n",
        "        image = Image.new('L', (image_width, image_height), 0)\n",
        "        draw = ImageDraw.Draw(image)\n",
        "\n",
        "        # Iterate over features\n",
        "        for feature in data['features']['xy']:\n",
        "            # Extract polygon coordinates\n",
        "            coordinates = feature['wkt'].replace('POLYGON ((', '').replace('))', '').split(', ')\n",
        "            points = []\n",
        "            for coordinate in coordinates:\n",
        "                x, y = coordinate.split(' ')\n",
        "                points.append((float(x), float(y)))\n",
        "            class_number = get_class_number(feature['properties']['feature_type'], feature['properties'].get('subtype'))\n",
        "            draw.polygon(points, fill=class_number)\n",
        "\n",
        "\n",
        "\n",
        "        # Save the image with the same name as the label file\n",
        "        output_filename = os.path.splitext(filename)[0] + '.png'\n",
        "        output_path = os.path.join(output_dir, output_filename)\n",
        "        image.save(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCg2XZt9hc4L"
      },
      "outputs": [],
      "source": [
        "#@title Flush `Google Drive` to save processed dataset. { display-mode: \"form\" }\n",
        "drive.flush_and_unmount()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WBW-1Q00Se7f"
      },
      "source": [
        "## Creating a TensorFlow Dataset\n",
        "\n",
        "Training on the entire xView2 dataset might take a lot of time, hence we can define a subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNzVxjxZSe7g"
      },
      "outputs": [],
      "source": [
        "#@title Create a subset of the dataset. { display-mode: \"form\" }\n",
        "IMAGE_SIZE = 1024\n",
        "BATCH_SIZE = 2 # Max 2, 3 gives OOM with 15GB VRAM\n",
        "NUM_CLASSES = 7\n",
        "DATA_DIR = \"Xview2/\" #@param {type:\"string\"}\n",
        "DATA_DIR = colab_drive+DATA_DIR\n",
        "NUM_TRAIN_IMAGES = 3998 #@param {type:\"number\"}\n",
        "NUM_VAL_IMAGES = 600 #@param {type:\"number\"}\n",
        "NUM_TEST_IMAGES = 1000 #@param {type:\"number\"}\n",
        "\n",
        "images_directories = sorted(glob(os.path.join(DATA_DIR, \"images/*\")))\n",
        "label_directories = sorted(glob(os.path.join(DATA_DIR, \"labelsdeeplab/*\")))\n",
        "\n",
        "discard_before_images = True #@param {type:\"boolean\"}\n",
        "shuffle_dataset = True #@param {type:\"boolean\"}\n",
        "\n",
        "if discard_before_images:\n",
        "    # Discard every other image and label before the first one\n",
        "    images_directories = images_directories[1::2]\n",
        "    label_directories = label_directories[1::2]\n",
        "    NUM_TRAIN_IMAGES //= 2\n",
        "    NUM_VAL_IMAGES //= 2\n",
        "    NUM_TEST_IMAGES //= 2\n",
        "\n",
        "if shuffle_dataset:\n",
        "    # Shuffle the image and label directories in sync\n",
        "    combined = list(zip(images_directories, label_directories))\n",
        "    random.shuffle(combined)\n",
        "    images_directories, label_directories = zip(*combined)\n",
        "\n",
        "train_images = images_directories[:NUM_TRAIN_IMAGES]\n",
        "train_masks = label_directories[:NUM_TRAIN_IMAGES]\n",
        "val_images = images_directories[\n",
        "    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n",
        "]\n",
        "val_masks = label_directories[\n",
        "    NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES\n",
        "]\n",
        "test_images = images_directories[\n",
        "    NUM_VAL_IMAGES + NUM_TRAIN_IMAGES: NUM_VAL_IMAGES + NUM_TRAIN_IMAGES + NUM_TEST_IMAGES\n",
        "]\n",
        "test_masks = label_directories[\n",
        "    NUM_VAL_IMAGES + NUM_TRAIN_IMAGES : NUM_VAL_IMAGES + NUM_TRAIN_IMAGES + NUM_TEST_IMAGES\n",
        "]\n",
        "\n",
        "\n",
        "def read_image(image_path, mask=False):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    if mask:\n",
        "        image = tf.image.decode_png(image, channels=1)\n",
        "        image.set_shape([None, None, 1])\n",
        "        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "    else:\n",
        "        image = tf.image.decode_png(image, channels=3)\n",
        "        image.set_shape([None, None, 3])\n",
        "        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "        image = tf.keras.applications.resnet50.preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_data(image_list, mask_list):\n",
        "    image = read_image(image_list)\n",
        "    mask = read_image(mask_list, mask=True)\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def data_generator(image_list, mask_list):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n",
        "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_dataset = data_generator(sorted(train_images), sorted(train_masks))\n",
        "val_dataset = data_generator(sorted(val_images), sorted(val_masks))\n",
        "test_dataset = data_generator(sorted(test_images), sorted(test_masks))\n",
        "\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Val Dataset:\", val_dataset)\n",
        "print(\"Test Dataset:\", test_dataset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "awfzEUL-Se7h"
      },
      "source": [
        "## Building the DeepLabV3+ model\n",
        "\n",
        "DeepLabv3+ extends DeepLabv3 by adding an encoder-decoder structure. The encoder module\n",
        "processes multiscale contextual information by applying dilated convolution at multiple\n",
        "scales, while the decoder module refines the segmentation results along object boundaries.\n",
        "\n",
        "![](https://github.com/lattice-ai/DeepLabV3-Plus/raw/master/assets/deeplabv3_plus_diagram.png)\n",
        "\n",
        "**Dilated convolution:** With dilated convolution, as we go deeper in the network, we can keep the\n",
        "stride constant but with larger field-of-view without increasing the number of parameters\n",
        "or the amount of computation. Besides, it enables larger output feature maps, which is\n",
        "useful for semantic segmentation.\n",
        "\n",
        "The reason for using **Dilated Spatial Pyramid Pooling** is that it was shown that as the\n",
        "sampling rate becomes larger, the number of valid filter weights (i.e., weights that\n",
        "are applied to the valid feature region, instead of padded zeros) becomes smaller."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RnH5HXCGSe7i"
      },
      "outputs": [],
      "source": [
        "#@title Defining the Dilated Convolution\n",
        "def convolution_block(\n",
        "    block_input,\n",
        "    num_filters=256,\n",
        "    kernel_size=3,\n",
        "    dilation_rate=1,\n",
        "    padding=\"same\",\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=dilation_rate,\n",
        "        padding=\"same\",\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=keras.initializers.HeNormal(),\n",
        "    )(block_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def DilatedSpatialPyramidPooling(dspp_input):\n",
        "    dims = dspp_input.shape\n",
        "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
        "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
        "    out_pool = layers.UpSampling2D(\n",
        "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "\n",
        "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
        "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
        "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
        "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
        "    output = convolution_block(x, kernel_size=1)\n",
        "    return output\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R2pR8cmRSe7i"
      },
      "source": [
        "The encoder features are first bilinearly upsampled by a factor 4, and then\n",
        "concatenated with the corresponding low-level features from the network backbone that\n",
        "have the same spatial resolution. For this example, we\n",
        "use a ResNet50 pretrained on ImageNet as the backbone model, and we use\n",
        "the low-level features from the `conv4_block6_2_relu` block of the backbone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "toyT41kiSe7j"
      },
      "outputs": [],
      "source": [
        "#@title Creating the model\n",
        "def DeeplabV3Plus(image_size, num_classes):\n",
        "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
        "    resnet50 = keras.applications.ResNet50(\n",
        "        weights=\"imagenet\", include_top=False, input_tensor=model_input\n",
        "    )\n",
        "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
        "    x = DilatedSpatialPyramidPooling(x)\n",
        "\n",
        "    input_a = layers.UpSampling2D(\n",
        "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
        "    x = convolution_block(x)\n",
        "    x = convolution_block(x)\n",
        "    x = layers.UpSampling2D(\n",
        "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
        "    return keras.Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "\n",
        "model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9RCJyc1VSe7j"
      },
      "source": [
        "## Training\n",
        "\n",
        "We train the model using sparse categorical crossentropy as the loss function, and\n",
        "Adam as the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vpgXZi-YPP7E"
      },
      "outputs": [],
      "source": [
        "#@title Define the history plotting\n",
        "def plot_history(history):\n",
        "    plt.plot(history[\"loss\"])\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(history[\"accuracy\"])\n",
        "    plt.title(\"Training Accuracy\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(history[\"val_loss\"])\n",
        "    plt.title(\"Validation Loss\")\n",
        "    plt.ylabel(\"val_loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(history[\"val_accuracy\"])\n",
        "    plt.title(\"Validation Accuracy\")\n",
        "    plt.ylabel(\"val_accuracy\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qMMgpl5FSe7j"
      },
      "outputs": [],
      "source": [
        "#@title Train the model\n",
        "epoch = 20 #@param {type:\"number\"}\n",
        "\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=loss,\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "now = datetime.now()\n",
        "dt_string = now.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
        "\n",
        "continue_training_from_folder = False #@param {type:\"boolean\"}\n",
        "if continue_training_from_folder:\n",
        "    continue_folder = \"folder name\" #@param {type:\"string\"}\n",
        "    dt_string = continue_folder\n",
        "\n",
        "initial_epoch = 0 #@param {type:\"integer\"}\n",
        "\n",
        "os.makedirs(colab_drive+'checkpoints/'+dt_string+'/', exist_ok=True)\n",
        "\n",
        "csv_logger = CSVLogger(colab_drive+'checkpoints/'+dt_string+'/history.csv', append=True)\n",
        "\n",
        "mc = keras.callbacks.ModelCheckpoint(colab_drive+'checkpoints/'+dt_string+'/model{epoch:08d}.h5',\n",
        "                                     save_weights_only=False, save_freq=1)\n",
        "\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, initial_epoch=initial_epoch, epochs=epoch, callbacks=[mc, csv_logger])\n",
        "\n",
        "# convert the history.history dict to a pandas DataFrame:\n",
        "hist_df = pd.DataFrame(history.history)\n",
        "\n",
        "# save to json:\n",
        "hist_json_file = colab_drive+'checkpoints/'+dt_string+'complete_history.json'\n",
        "with open(hist_json_file, mode='w') as f:\n",
        "    hist_df.to_json(f)\n",
        "\n",
        "# or save to csv:\n",
        "hist_csv_file = colab_drive+'checkpoints/'+dt_string+'complete_history.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)\n",
        "\n",
        "plot_history(history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Fe4Mmhq92jup"
      },
      "outputs": [],
      "source": [
        "#@title Manually save the model\n",
        "model.save(\"my_model.h5\")\n",
        "shutil.copy('/content/my_model.h5', colab_drive+'checkpoints/'+dt_string+'last_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_Y3pa3_Vcp4"
      },
      "outputs": [],
      "source": [
        "#@title Disconnect colab after training. { display-mode: \"form\" }\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZRTiZjBYq4Dr"
      },
      "outputs": [],
      "source": [
        "#@title Load the train history and visualize\n",
        "history_path = \"checkpoints/history.csv\" #@param {type:\"string\"}\n",
        "loaded_history = pd.read_csv(colab_drive+history_path)\n",
        "\n",
        "plot_history(loaded_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fZAnSR7XlCvf"
      },
      "outputs": [],
      "source": [
        "#@title Load the model\n",
        "checkpoint_path = \"checkpoints/use_final.h5\" #@param {type:\"string\"}\n",
        "\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model = tf.keras.models.load_model(colab_drive+checkpoint_path, compile=False)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=loss,\n",
        "    metrics=[\"accuracy\"],)\n",
        "# Check its architecture\n",
        "model.summary()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jZHADrkcSe7k"
      },
      "source": [
        "## Inference using Colormap Overlay\n",
        "\n",
        "The raw predictions from the model represent a one-hot encoded tensor.\n",
        "In order to visualize the results, we plot them as RGB segmentation masks where each pixel is represented by a unique color corresponding to the particular label predicted. We would also plot an overlay of the RGB segmentation mask on the input image as this further helps us to identify the different categories present in the image more intuitively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NQxRBmH6Se7k"
      },
      "outputs": [],
      "source": [
        "#@title Define inference functions\n",
        "\n",
        "# Loading the Colormap\n",
        "colormap = np.array([\n",
        " [255, 255,  255], # White\n",
        " [128, 128, 128], # Gray\n",
        " [0, 128, 0],     # Green\n",
        " [255, 165, 0],   # Orange\n",
        " [0, 255, 255],   # Cyan\n",
        " [255, 0, 0],     # Red\n",
        " [255, 192, 203]  # Pink\n",
        " ])\n",
        "colormap = colormap.astype(np.uint8)\n",
        "\n",
        "\n",
        "def infer(model, image_tensor):\n",
        "    predictions = model.predict(np.expand_dims((image_tensor), axis=0))\n",
        "    predictions = np.squeeze(predictions)\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "def decode_segmentation_masks(mask, colormap, n_classes):\n",
        "    r = np.zeros_like(mask).astype(np.uint8)\n",
        "    g = np.zeros_like(mask).astype(np.uint8)\n",
        "    b = np.zeros_like(mask).astype(np.uint8)\n",
        "    for l in range(0, n_classes):\n",
        "        idx = mask == l\n",
        "        r[idx] = colormap[l, 0]\n",
        "        g[idx] = colormap[l, 1]\n",
        "        b[idx] = colormap[l, 2]\n",
        "    rgb = np.stack([r, g, b], axis=2)\n",
        "    return rgb\n",
        "\n",
        "\n",
        "def get_overlay(image, colored_mask):\n",
        "    image = tf.keras.utils.array_to_img(image)\n",
        "    image = np.array(image).astype(np.uint8)\n",
        "    overlay = cv2.addWeighted(image, 0.35, colored_mask, 0.65, 0)\n",
        "    return overlay\n",
        "\n",
        "\n",
        "def plot_samples_matplotlib(display_list, figsize=(5, 3)):\n",
        "    _, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)\n",
        "    for i in range(len(display_list)):\n",
        "        if display_list[i].shape[-1] == 3:\n",
        "            axes[i].imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
        "        else:\n",
        "            axes[i].imshow(display_list[i])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_predictions(images_list, colormap, model):\n",
        "    for image_file in images_list:\n",
        "        image_tensor = read_image(image_file)\n",
        "        prediction_mask = infer(image_tensor=image_tensor, model=model)\n",
        "        prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, NUM_CLASSES)\n",
        "        overlay = get_overlay(image_tensor, prediction_colormap)\n",
        "        plot_samples_matplotlib(\n",
        "            [image_tensor, overlay, prediction_colormap], figsize=(18, 14)\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AS0fKZDHSe7l"
      },
      "outputs": [],
      "source": [
        "#@title Inference on Train Images\n",
        "\n",
        "ind = randrange(len(train_images)-8)\n",
        "plot_predictions(train_images[ind:ind+4], colormap, model=model)\n",
        "plot_predictions(train_images[ind+4:ind+8], colormap, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QMlT4uzESe7l"
      },
      "outputs": [],
      "source": [
        "#@title Inference on Validation Images\n",
        "val_ind = randrange(len(val_images)-8)\n",
        "plot_predictions(val_images[val_ind:val_ind+4], colormap, model=model)\n",
        "plot_predictions(val_images[val_ind+4:val_ind+8], colormap, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "btCi1G_pPP7I"
      },
      "outputs": [],
      "source": [
        "#@title Inference on Test Images\n",
        "test_ind = randrange(len(test_images)-8)\n",
        "plot_predictions(test_images[test_ind:test_ind+4], colormap, model=model)\n",
        "plot_predictions(test_images[test_ind+4:test_ind+8], colormap, model=model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
